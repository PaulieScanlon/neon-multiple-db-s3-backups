name: fincorp-billing-prod

on:
  schedule:
    # Runs at midnight ET (us-east-1)
    - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  db-backup:
    runs-on: ubuntu-latest

    permissions:
      id-token: write

    env:
      RETENTION: 1
      DATABASE_URL: ${{ secrets.FINCORP_BILLING_PROD }}
      IAM_ROLE: ${{ secrets.IAM_ROLE }}
      AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
      S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
      BACKUP_NAME: ${{ github.workflow }}
      PG_VERSION: '16'
      AWS_REGION: 'us-east-1'

    steps:
      - name: Install PostgreSQL
        run: |
          sudo apt install -y postgresql-common
          yes '' | sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh
          sudo apt install -y postgresql-${{ env.PG_VERSION }}

      - name: Get timestamp
        id: timestamp
        run: |
          echo "TIMESTAMP=$(date +%d-%B-%Y@%H:%M:%S)" >> $GITHUB_ENV

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ env.AWS_ACCOUNT_ID }}:role/${{ env.IAM_ROLE }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Create Folder if it doesn't exist
        run: |
          FOLDER="${{ env.BACKUP_NAME }}/"
          if ! aws s3api head-object --bucket ${{ env.S3_BUCKET_NAME }} --key "$FOLDER" 2>/dev/null; then
            echo "Folder does not exist. Creating folder: $FOLDER"
            aws s3api put-object --bucket ${{ env.S3_BUCKET_NAME }} --key "$FOLDER"
          fi

      - name: Run pg_dump
        run: |
          TIMESTAMP="${{ env.TIMESTAMP }}"
          FILENAME="${TIMESTAMP}.sql.gz"
          /usr/lib/postgresql/${{ env.PG_VERSION }}/bin/pg_dump ${{ env.DATABASE_URL }} | gzip > "${FILENAME}"
          echo "FILENAME=${FILENAME}" >> $GITHUB_ENV

      - name: Empty Bucket of Old Files
        run: |
          THRESHOLD_DATE=$(date -d "-${{ env.RETENTION }} days" +%Y-%m-%dT%H:%M:%SZ)
          aws s3api list-objects --bucket ${{ env.S3_BUCKET_NAME }} --query "Contents[?LastModified<'${THRESHOLD_DATE}'] | [?contains(Key, '${{ env.BACKUP_NAME }}')].{Key: Key}" --output text | while read -r file; do
            echo "Deleting old file: $file"
            aws s3 rm "s3://${{ env.S3_BUCKET_NAME }}/$file"
          done

      - name: Upload to Bucket
        run: |
          FILENAME="${{ env.FILENAME }}"
          FOLDER="${{ env.BACKUP_NAME }}/"
          echo "Uploading ${FILENAME} to s3://${{ env.S3_BUCKET_NAME }}/$FOLDER"
          aws s3 cp "${FILENAME}" "s3://${{ env.S3_BUCKET_NAME }}/$FOLDER" --region ${{ env.AWS_REGION }}
